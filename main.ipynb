{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from rlgym.policy import Policy\n",
    "\n",
    "plt.style.use(\"bmh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DISCRETE ENV\n",
    "# env_name = \"CartPole-v1\"\n",
    "# env_name = \"LunarLander-v2\"\n",
    "## CONTINUOUS ENV\n",
    "# env_name = \"Pendulum-v1\"\n",
    "env_name = \"LunarLanderContinuous-v2\"\n",
    "# env_name = \"MountainCarContinuous-v0\"\n",
    "\n",
    "env = gym.make(env_name, new_step_api=True)\n",
    "n_episode = 1500\n",
    "log_every_n = n_episode / 100\n",
    "x = np.arange(0, n_episode, log_every_n)\n",
    "\n",
    "\n",
    "def train(algo,\n",
    "          learning_rate,\n",
    "          hidden_size,\n",
    "          number_of_layers,\n",
    "          shared_layers=True):\n",
    "\n",
    "    obversation_space = env.observation_space.shape[0]\n",
    "    action_space = env.action_space\n",
    "    action_space_type = type(env.action_space).__name__.lower()\n",
    "\n",
    "    if action_space_type == \"discrete\":\n",
    "        is_continuous = False\n",
    "    elif action_space_type == \"box\":\n",
    "        is_continuous = True\n",
    "    else:\n",
    "        raise Exception(\"Unvalid type of action_space\")\n",
    "\n",
    "    policy = Policy(algo, obversation_space, action_space, is_continuous,\n",
    "                    learning_rate, hidden_size, number_of_layers,\n",
    "                    shared_layers)\n",
    "\n",
    "    log_rewards = []\n",
    "\n",
    "    for t in tqdm(range(n_episode)):\n",
    "\n",
    "        current_state = env.reset()\n",
    "        next_state = None\n",
    "        states, actions, next_states, rewards, flags, logprobs = [], [], [], [], [], []\n",
    "        terminated, truncated = False, False\n",
    "\n",
    "        while not terminated and not truncated:\n",
    "            action, log_prob = policy.get_action(current_state)\n",
    "\n",
    "            if is_continuous:\n",
    "                action = np.array(action, ndmin=1)\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            states.append(current_state)\n",
    "            actions.append(action)\n",
    "            next_states.append(next_state)\n",
    "            rewards.append(reward)\n",
    "            flags.append(int(terminated))\n",
    "            logprobs.append(log_prob)\n",
    "\n",
    "            current_state = next_state\n",
    "\n",
    "        policy.update_policy({\n",
    "            \"states\":\n",
    "            torch.from_numpy(np.array(states)).to(torch.device(\"cuda\")),\n",
    "            \"actions\":\n",
    "            torch.from_numpy(np.array(actions)).to(torch.device(\"cuda\")),\n",
    "            \"next_states\":\n",
    "            torch.from_numpy(np.array(next_states)).to(torch.device(\"cuda\")),\n",
    "            \"rewards\":\n",
    "            torch.from_numpy(np.array(rewards)).to(torch.device(\"cuda\")),\n",
    "            \"flags\":\n",
    "            torch.from_numpy(np.array(flags)).to(torch.device(\"cuda\")),\n",
    "            \"logprobs\":\n",
    "            torch.stack(logprobs).squeeze()\n",
    "        })\n",
    "\n",
    "        if not t % log_every_n:\n",
    "            log_rewards.append(np.sum(rewards))\n",
    "\n",
    "    # policy.save(\"model-\" + env_name + \"-\" + algo + \".pt\")\n",
    "\n",
    "    return log_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Reinforce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_rewards = train(algo=\"reinforce\",\n",
    "                    learning_rate=3e-3,\n",
    "                    hidden_size=128,\n",
    "                    number_of_layers=3)\n",
    "\n",
    "plt.plot(x, log_rewards)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_rewards = train(algo=\"reinforce\",\n",
    "                    learning_rate=1e-3,\n",
    "                    hidden_size=128,\n",
    "                    number_of_layers=3)\n",
    "\n",
    "plt.plot(x, log_rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_rewards = train(algo=\"reinforce\",\n",
    "                    learning_rate=1e-4,\n",
    "                    hidden_size=128,\n",
    "                    number_of_layers=3)\n",
    "\n",
    "plt.plot(x, log_rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# A2C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_rewards = train(algo=\"a2c\",\n",
    "                    learning_rate=3e-3,\n",
    "                    hidden_size=128,\n",
    "                    number_of_layers=3)\n",
    "\n",
    "plt.plot(x, log_rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_rewards = train(algo=\"a2c\",\n",
    "                    learning_rate=1e-3,\n",
    "                    hidden_size=128,\n",
    "                    number_of_layers=3)\n",
    "\n",
    "plt.plot(x, log_rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_rewards = train(algo=\"a2c\",\n",
    "                    learning_rate=3e-4,\n",
    "                    hidden_size=128,\n",
    "                    number_of_layers=3)\n",
    "\n",
    "plt.plot(x, log_rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PPO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_rewards = train(algo=\"ppo\",\n",
    "                    learning_rate=1e-3,\n",
    "                    hidden_size=128,\n",
    "                    number_of_layers=3)\n",
    "\n",
    "plt.plot(x, log_rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_rewards = train(algo=\"ppo\",\n",
    "                    learning_rate=3e-4,\n",
    "                    hidden_size=128,\n",
    "                    number_of_layers=3)\n",
    "\n",
    "plt.plot(x, log_rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_rewards = train(algo=\"ppo\",\n",
    "                    learning_rate=1e-4,\n",
    "                    hidden_size=128,\n",
    "                    number_of_layers=3)\n",
    "\n",
    "plt.plot(x, log_rewards)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a764b75b50eda8fbd40025cb02341fcbad10a66e8df2567e0aa082938d940e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
