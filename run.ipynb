{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from rlgym.policy import Policy\n",
    "\n",
    "plt.style.use(\"bmh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "box2D is not installed, run `pip install gym[box2d]`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/envs/box2d/bipedal_walker.py:14\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 14\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mBox2D\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mBox2D\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mb2\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m         circleShape,\n\u001b[1;32m     17\u001b[0m         contactListener,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m         revoluteJointDef,\n\u001b[1;32m     22\u001b[0m     )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Box2D'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m env_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mLunarLanderContinuous-v2\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[39m# env_name = \"MountainCarContinuous-v0\"\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(env_name)\n\u001b[1;32m     10\u001b[0m n_episode \u001b[39m=\u001b[39m \u001b[39m1500\u001b[39m\n\u001b[1;32m     11\u001b[0m log_every_n \u001b[39m=\u001b[39m n_episode \u001b[39m/\u001b[39m \u001b[39m100\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/envs/registration.py:581\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m     env_creator \u001b[39m=\u001b[39m spec_\u001b[39m.\u001b[39mentry_point\n\u001b[1;32m    579\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    580\u001b[0m     \u001b[39m# Assume it's a string\u001b[39;00m\n\u001b[0;32m--> 581\u001b[0m     env_creator \u001b[39m=\u001b[39m load(spec_\u001b[39m.\u001b[39;49mentry_point)\n\u001b[1;32m    583\u001b[0m mode \u001b[39m=\u001b[39m _kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrender_mode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    584\u001b[0m apply_human_rendering \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/envs/registration.py:61\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39m\"\"\"Loads an environment with name and returns an environment creation function\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39m    Calls the environment constructor\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m mod_name, attr_name \u001b[39m=\u001b[39m name\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m mod \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(mod_name)\n\u001b[1;32m     62\u001b[0m fn \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(mod, attr_name)\n\u001b[1;32m     63\u001b[0m \u001b[39mreturn\u001b[39;00m fn\n",
      "File \u001b[0;32m/usr/lib64/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:992\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/envs/box2d/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbox2d\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbipedal_walker\u001b[39;00m \u001b[39mimport\u001b[39;00m BipedalWalker, BipedalWalkerHardcore\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbox2d\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcar_racing\u001b[39;00m \u001b[39mimport\u001b[39;00m CarRacing\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menvs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbox2d\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlunar_lander\u001b[39;00m \u001b[39mimport\u001b[39;00m LunarLander, LunarLanderContinuous\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/envs/box2d/bipedal_walker.py:24\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mBox2D\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mb2\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m         circleShape,\n\u001b[1;32m     17\u001b[0m         contactListener,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m         revoluteJointDef,\n\u001b[1;32m     22\u001b[0m     )\n\u001b[1;32m     23\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m     \u001b[39mraise\u001b[39;00m DependencyNotInstalled(\u001b[39m\"\u001b[39m\u001b[39mbox2D is not installed, run `pip install gym[box2d]`\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[39mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m     28\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mpygame\u001b[39;00m\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m: box2D is not installed, run `pip install gym[box2d]`"
     ]
    }
   ],
   "source": [
    "## DISCRETE ENV\n",
    "# env_name = \"CartPole-v1\"\n",
    "# env_name = \"LunarLander-v2\"\n",
    "## CONTINUOUS ENV\n",
    "# env_name = \"Pendulum-v1\"\n",
    "env_name = \"LunarLanderContinuous-v2\"\n",
    "# env_name = \"MountainCarContinuous-v0\"\n",
    "\n",
    "env = gym.make(env_name)\n",
    "n_episode = 1500\n",
    "log_every_n = n_episode / 100\n",
    "x = np.arange(0, n_episode, log_every_n)\n",
    "\n",
    "\n",
    "def train(algo,\n",
    "          learning_rate,\n",
    "          hidden_size,\n",
    "          number_of_layers,\n",
    "          is_shared_network=True):\n",
    "\n",
    "    obversation_space = env.observation_space.shape[0]\n",
    "    action_space = env.action_space\n",
    "\n",
    "    policy = Policy(algo, obversation_space, action_space, learning_rate,\n",
    "                    hidden_size, number_of_layers, is_shared_network)\n",
    "\n",
    "    log_rewards = []\n",
    "\n",
    "    for t in tqdm(range(n_episode)):\n",
    "\n",
    "        current_state, _ = env.reset()\n",
    "        next_state = None\n",
    "        states, actions, next_states, rewards, flags, logprobs = [], [], [], [], [], []\n",
    "        terminated, truncated = False, False\n",
    "\n",
    "        while not terminated and not truncated:\n",
    "            action, log_prob = policy.get_action(current_state)\n",
    "\n",
    "            if policy.is_continuous:\n",
    "                action = np.array(action, ndmin=1)\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            states.append(current_state)\n",
    "            actions.append(action)\n",
    "            next_states.append(next_state)\n",
    "            rewards.append(reward)\n",
    "            flags.append(int(terminated))\n",
    "            logprobs.append(log_prob)\n",
    "\n",
    "            current_state = next_state\n",
    "\n",
    "        policy.update_policy({\n",
    "            \"states\":\n",
    "            torch.from_numpy(np.array(states)).to(torch.device(\"cuda\")),\n",
    "            \"actions\":\n",
    "            torch.from_numpy(np.array(actions)).to(torch.device(\"cuda\")),\n",
    "            \"next_states\":\n",
    "            torch.from_numpy(np.array(next_states)).to(torch.device(\"cuda\")),\n",
    "            \"rewards\":\n",
    "            torch.from_numpy(np.array(rewards)).to(torch.device(\"cuda\")),\n",
    "            \"flags\":\n",
    "            torch.from_numpy(np.array(flags)).to(torch.device(\"cuda\")),\n",
    "            \"logprobs\":\n",
    "            torch.stack(logprobs).squeeze()\n",
    "        })\n",
    "\n",
    "        if not t % log_every_n:\n",
    "            log_rewards.append(np.sum(rewards))\n",
    "\n",
    "    # policy.save(\"model-\" + env_name + \"-\" + algo + \".pt\")\n",
    "\n",
    "    return log_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Reinforce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_rewards = train(algo=\"reinforce\",\n",
    "                    learning_rate=3e-3,\n",
    "                    hidden_size=128,\n",
    "                    number_of_layers=3)\n",
    "\n",
    "plt.plot(x, log_rewards)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_rewards = train(algo=\"reinforce\",\n",
    "                    learning_rate=1e-3,\n",
    "                    hidden_size=128,\n",
    "                    number_of_layers=3)\n",
    "\n",
    "plt.plot(x, log_rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_rewards = train(algo=\"reinforce\",\n",
    "                    learning_rate=1e-4,\n",
    "                    hidden_size=128,\n",
    "                    number_of_layers=3)\n",
    "\n",
    "plt.plot(x, log_rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# A2C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_rewards = train(algo=\"a2c\",\n",
    "                    learning_rate=3e-3,\n",
    "                    hidden_size=128,\n",
    "                    number_of_layers=3)\n",
    "\n",
    "plt.plot(x, log_rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_rewards = train(algo=\"a2c\",\n",
    "                    learning_rate=1e-3,\n",
    "                    hidden_size=128,\n",
    "                    number_of_layers=3)\n",
    "\n",
    "plt.plot(x, log_rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_rewards = train(algo=\"a2c\",\n",
    "                    learning_rate=3e-4,\n",
    "                    hidden_size=128,\n",
    "                    number_of_layers=3)\n",
    "\n",
    "plt.plot(x, log_rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PPO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_rewards = train(algo=\"ppo\",\n",
    "                    learning_rate=1e-3,\n",
    "                    hidden_size=128,\n",
    "                    number_of_layers=3)\n",
    "\n",
    "plt.plot(x, log_rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_rewards = train(algo=\"ppo\",\n",
    "                    learning_rate=3e-4,\n",
    "                    hidden_size=128,\n",
    "                    number_of_layers=3)\n",
    "\n",
    "plt.plot(x, log_rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_rewards = train(algo=\"ppo\",\n",
    "                    learning_rate=1e-4,\n",
    "                    hidden_size=128,\n",
    "                    number_of_layers=3)\n",
    "\n",
    "plt.plot(x, log_rewards)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('rlgym-8JaFr494-py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b7f49c8375c6959f4edc7abe0cd0f0d5ae13432d1617e192aa21026d1dbfc45"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
